{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI applications using MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os \n",
    "# from langchain.chat_models import AzureChatOpenAI\n",
    "# from openai import AzureOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "import dagshub\n",
    "import pandas as pd \n",
    "import openai \n",
    "\n",
    "# load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test data \n",
    "eval_data=pd.DataFrame(\n",
    "    {\n",
    "        'inputs':[\n",
    "            \"what is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\":[\n",
    "            \"MLflow is an open-source platform designed to manage the machine learning (ML) lifecycle, which includes experimentation, reproducibility, and deployment of ML models. It was developed by Databricks to address the challenges of managing ML projects and has since become a widely adopted tool in the ML community.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It provides a fast and general-purpose engine for processing large-scale data with high efficiency and scalability.\"\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as Immortal-Pi\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as Immortal-Pi\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"Immortal-Pi/mlflow-genAI-test\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"Immortal-Pi/mlflow-genAI-test\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository Immortal-Pi/mlflow-genAI-test initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository Immortal-Pi/mlflow-genAI-test initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='Immortal-Pi', repo_name='mlflow-genAI-test', mlflow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.11it/s]\n",
      "2025/01/18 20:29:46 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/01/18 20:29:51 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/01/18 20:29:51 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/01/18 20:29:51 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:51 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/01/18 20:29:51 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:51 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/01/18 20:29:51 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:51 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/01/18 20:29:51 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 5 in the `extra_metrics` parameter because it returned None.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.23s/it]\n",
      "2025/01/18 20:29:53 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/01/18 20:29:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 7 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:53 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/01/18 20:29:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 8 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:53 WARNING mlflow.metrics.metric_definitions: Failed to load 'bleu' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/01/18 20:29:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'bleu' at index 10 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:53 WARNING mlflow.metrics.metric_definitions: Failed to load 'rouge' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/01/18 20:29:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'rouge1' at index 11 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:53 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/01/18 20:29:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:53 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/01/18 20:29:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 2 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:53 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/01/18 20:29:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 3 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:53 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/01/18 20:29:53 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 5 in the `extra_metrics` parameter because it returned None.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.41s/it]\n",
      "2025/01/18 20:29:56 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/01/18 20:29:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'flesch_kincaid_grade_level' at index 7 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:56 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2025/01/18 20:29:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'ari_grade_level' at index 8 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:56 WARNING mlflow.metrics.metric_definitions: Failed to load 'bleu' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/01/18 20:29:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'bleu' at index 10 in the `extra_metrics` parameter because it returned None.\n",
      "2025/01/18 20:29:56 WARNING mlflow.metrics.metric_definitions: Failed to load 'rouge' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/01/18 20:29:56 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'rouge1' at index 11 in the `extra_metrics` parameter because it returned None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see the aggregate results below: \n",
      "{'latency/mean': 2.3138840198516846, 'latency/variance': 0.5617295759253125, 'latency/p90': 2.9134729862213136, 'exact_match/v1': 0.0, 'answer_similarity/v1/mean': 4.5, 'answer_similarity/v1/variance': 0.25, 'answer_similarity/v1/p90': 4.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see evaluation table below: \n",
      "            inputs                                       ground_truth  \\\n",
      "0  what is MLflow?  MLflow is an open-source platform designed to ...   \n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
      "\n",
      "                                             outputs   latency  token_count  \\\n",
      "0  MLflow is an open-source platform primarily de...  3.063370           68   \n",
      "1  Apache Spark is an open-source unified analyti...  1.564398           47   \n",
      "\n",
      "   answer_similarity/v1/score  \\\n",
      "0                           4   \n",
      "1                           5   \n",
      "\n",
      "                  answer_similarity/v1/justification  \n",
      "0  The output provided aligns closely with the ta...  \n",
      "1  The provided output closely aligns with the ta...  \n",
      "ðŸƒ View run useful-lark-132 at: https://dagshub.com/Immortal-Pi/mlflow-genAI-test.mlflow/#/experiments/0/runs/89c3ebbd06c74347968ad21de6949e9d\n",
      "ðŸ§ª View experiment at: https://dagshub.com/Immortal-Pi/mlflow-genAI-test.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment('LLM Evaluation')\n",
    "openai.api_type='azure'\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://llmops-amruth.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv('AZURE_OpenAI_API_VERSION')\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('GRAPHRAG_API_KEY')\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv('GRAPHRAG_API_KEY')\n",
    "with mlflow.start_run() as run:\n",
    "    system_prompt='Answer the following questions in 2 sentences'\n",
    "    logged_model_info=mlflow.openai.log_model(\n",
    "        model='gpt-4',\n",
    "        task=openai.chat.completions,\n",
    "        artifact_path='model',\n",
    "        messages=[\n",
    "            {'role':'system','content':system_prompt},\n",
    "            {'role':'user','content':\"{question}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    #use predefined question-answer metrics to evaluate our model\n",
    "    results=logged_model_info=mlflow.evaluate(\n",
    "        logged_model_info.model_uri,\n",
    "        eval_data,\n",
    "        targets='ground_truth',\n",
    "        model_type='question-answering',\n",
    "        extra_metrics=[mlflow.metrics.toxicity(),\n",
    "                       mlflow.metrics.latency(),\n",
    "                       mlflow.metrics.genai.answer_similarity(),\n",
    "                       mlflow.metrics.flesch_kincaid_grade_level(), \n",
    "                       mlflow.metrics.ari_grade_level(),\n",
    "                       mlflow.metrics.exact_match(),\n",
    "                       mlflow.metrics.bleu(),\n",
    "                       mlflow.metrics.rouge1()\n",
    "                       ]\n",
    "    )\n",
    "    print(f'see the aggregate results below: \\n{results.metrics}')\n",
    "\n",
    "    # Evaluation result for each data record is available in results.tables\n",
    "    eval_table=results.tables['eval_results_table']\n",
    "    df=pd.DataFrame(eval_table)\n",
    "    df.to_csv('eval.csv')\n",
    "    print(f'see evaluation table below: \\n{eval_table}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
